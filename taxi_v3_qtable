import numpy as np
import gym
import random

env = gym.make('Taxi-v3')
env.render()
action_size = env.action_space.n;
state_size  =env.observation_space.n;

print("state_size", state_size)
print("action_size", action_size)

# create Q table
q_table = np.zeros([state_size,action_size]);

total_episodes = 5000;
total_test_episodes = 100;
max_steps = 99;
learning_rate = 0.7;
gamma = 0.618;

# Exploration Parameters

epsilon =1;
max_epsilon = 1;
min_epsilon = 0.01;
decay_rate = 0.01;
count = 0

for episode in range(total_episodes):
    state = env.reset()
    step = 0;
    done = False
    count = count + 1
    
    for step in range(max_steps):
        exp_exp_tradeoff = random.uniform(0,1)
        
        if exp_exp_tradeoff > epsilon:
            action = np.argmax(q_table[state,:])
            
        else:
            action = env.action_space.sample()
            
        new_state, reward, done, info = env.step(action)
        q_table[state, action] = q_table[state, action] + learning_rate * (reward + gamma * np.max(q_table[new_state,:]) - q_table[state, action])
        
        state = new_state
        
        if done == True:
            break
    episode = episode + 1
    epsilon = min_epsilon +(max_epsilon -min_epsilon)*np.exp(-decay_rate*episode)
    print(epsilon)
# Test

env.reset()
rewards = []

for episode in range(total_test_episodes):
    state = env.reset()
    step = 0
    done = False
    total_rewards = 0
    print("-------------------------------")
    print("Episode, episode")
    
    for step in range(max_steps):
        env.render()
        action = np.argmax(q_table[state,:])
        new_state, reward, done, info = env.step(action)
        total_rewards = total_rewards +reward
        
        if done:
            rewards.append(total_rewards)
            break
        state = new_state
env.close()
average_reward = (sum(rewards)/total_test_episodes)
print("Score over time:")
